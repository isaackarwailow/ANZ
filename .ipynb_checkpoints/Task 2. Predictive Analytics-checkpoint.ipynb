{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analysis: ANZ Synthesized 3-month Transactional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import reverse_geocoder as rg\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Source: https://www.theforage.com/modules/ZLJCsrpkHo9pZBJNY/BiJPfqmGY2QwgN6gA\n",
    "\n",
    "This task is based on a synthesised transaction dataset containing 3 months’ worth of transactions for 100 hypothetical customers. It contains purchases, recurring transactions, and salary transactions. The dataset is designed to simulate realistic transaction behaviours that are observed in ANZ’s real transaction data, so many of the insights gathered from the activities will be genuine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load the dataset and perform EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The aim of this dataset is to try to understand the features of purchasing behaviour of customers (independent variables) for modelling expected values for annualized salary (dependent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referencing the cleaned dataset\n",
    "file = 'DATA/ANZ-synthesized-transactions-cleaned.csv'\n",
    "# Read file and parse timestamp as the index\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distinct values for each column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.txn_description.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merch_state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sns = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.boxplot(ax=ax[0], x='txn_description', y=np.log(df.amount), hue='gender', data=df_sns, palette=\"cool_r\")\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), size=9, rotation=30)\n",
    "sns.boxplot(ax=ax[1], x='merch_state', y=np.log(df.amount), data=df_sns, palette=\"cool_r\", width=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state_cat = df.groupby(['account', 'merch_state']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.sum(df_state_cat[['ACT', 'NSW', 'NT', 'QLD', 'SA', 'TAS', 'VIC', 'WA']].sum(axis=1) != 0) \\\n",
    "            == len(df.groupby('account')), 'If dataframe is indexed on customer accounts, \\\n",
    "            then there will be no missing values for merchant information'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Drop irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('status' or 'card_present_flag' or 'first_name' or 'country' or 'timestamp' \\\n",
    "    or 'geometry' or 'merch_suburb' or 'distance' or 'merch_geometry' or 'card_present_bool') in df:\n",
    "    df = df.drop(columns=['status', 'card_present_flag', 'first_name','country', \n",
    "                        'timestamp', 'geometry', 'merch_suburb', 'distance',\n",
    "                        'merch_geometry', 'card_present_bool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Conduct Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocoder(data):\n",
    "    '''\n",
    "    input: dataframe containing Latitude(x) and Longitude(y) coordinates\n",
    "    output: JSON data containing info on available building or street names.\n",
    "    '''\n",
    "    coordinates = list(zip(data.X, data.Y))\n",
    "    results = rg.search(coordinates) # default mode = 2\n",
    "    return results\n",
    "\n",
    "list_1 = geocoder(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes = pd.DataFrame.from_dict(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes.cc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AU = Australia\n",
    "* ZA = South Africa\n",
    "* AO = Angola\n",
    "* ID = Indonesia\n",
    "* AQ = Antartica\n",
    "* NA = Namibia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes.admin1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null = df_geocodes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes[~df_geocodes.admin1.str.contains('\\w')].admin1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geocodes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cust_state'] = df_geocodes['admin1']\n",
    "df['cc'] = df_geocodes['cc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy variables\n",
    "df = pd.get_dummies(data=df, columns=['gender', 'txn_description', 'merch_state', 'cust_state', 'cc'], \n",
    "                    prefix=['g', 't', 'm', 'c', 'cc'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineer from coordinates X and Y\n",
    "x = df.X\n",
    "y = df.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Source:** https://bmanikan.medium.com/feature-engineering-all-i-learned-about-geo-spatial-features-649871d16796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geospatial feature engineering trick 1: Add 4 new features of Polar coordinates to the dataset\n",
    "df['r'] = np.sqrt(x**2 + y**2)\n",
    "df['phi'] = np.arctan2(y, x)\n",
    "df['rot_x'] = np.cos(x) + np.sin(x)\n",
    "df['rot_y'] = np.sin(x) - np.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geospatial feature engineering trick 2: Add 4 new features of rotational Cartesian coordinates\n",
    "\n",
    "df['rot_45_x'] = (0.707 * x) + (0.707 * y)\n",
    "df['rot_45_y'] = (0.707 * y) + (0.707 * x)\n",
    "df['rot_30_x'] = (0.866 * x) + (0.5 * y)\n",
    "df['rot_30_y'] = (0.866 * y) + (0.5 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geospatial feature engineering trick 3: Add new feature of Haversine distance to the dataset\n",
    "x_1 = df.merch_X\n",
    "y_1 = df.merch_Y\n",
    "\n",
    "def haversine_dist(lat1, lon1, lat2, lon2):\n",
    "   r = 6371\n",
    "   phi1 = np.radians(lat1)\n",
    "   phi2 = np.radians(lat2)\n",
    "   delta_phi = np.radians(lat2 - lat1)\n",
    "   delta_lambda = np.radians(lon2 - lon1)\n",
    "   a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n",
    "   res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "   return np.round(res, 2)\n",
    "\n",
    "df['h_dist'] = haversine_dist(x, y, x_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_dist(lat1, lng1, lat2, lng2):\n",
    "    '''\n",
    "    calculating two haversine distances by,\n",
    "     - avoiding Latitude of one point \n",
    "     - avoiding Longitude of one point\n",
    "    and adding it together.\n",
    "    '''\n",
    "    a = haversine_dist(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_dist(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "df['m_dist'] = manhattan_dist(x, y, x_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearing_degree(lat1, lng1, lat2, lng2):\n",
    "    '''\n",
    "    calculate angle between two points\n",
    "    '''\n",
    "    radius = 6371  # Mean radius of Earth\n",
    "    diff_lng = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(diff_lng) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(diff_lng)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "df['b_deg'] = bearing_degree(x, y, x_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data):\n",
    "    '''\n",
    "    input: dataframe containing Latitude(x) and Longitude(y)\n",
    "    '''\n",
    "    coordinates = data[['X','Y']].values\n",
    "    pca_obj = PCA(random_state=42).fit(coordinates)\n",
    "    pca_x = pca_obj.transform(data[['X', 'Y']])[:,0]\n",
    "    pca_y = pca_obj.transform(data[['X', 'Y']])[:,1]\n",
    "    return pca_x, pca_y\n",
    "\n",
    "df['pca_x'], df['pca_y'] = pca(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['X', 'Y', 'merch_X', 'merch_Y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'account': 'acc', 'balance': 'bal', 'amount': 'amt','t_PAY/SALARY': 't_pay', 't_PAYMENT': 't_pmt', \n",
    "                        't_PHONE BANK': 't_phb', 't_POS': 't_pos', 't_SALES-POS': 't_sales', 'c_Erongo': 'c_ER', \n",
    "                        'c_New South Wales': 'c_NSW', 'c_Queensland': 'c_QLD', 'c_South Australia': 'c_SA', \n",
    "                        'c_Sulawesi Tenggara': 'c_SULTRA', 'c_Tasmania': 'c_TAS', 'c_Victoria': 'c_VIC', \n",
    "                        'c_Western Australia': 'c_WA', 'c_Western Cape': 'c_WC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '_NSW' not in df: \n",
    "    df['_NSW'] = df['c_NSW'] + df['m_NSW']\n",
    "    df.drop(columns=['c_NSW', 'm_NSW'], inplace=True)\n",
    "if '_QLD' not in df:\n",
    "    df['_QLD'] = df['c_QLD'] + df['m_QLD']\n",
    "    df.drop(columns=['c_QLD', 'm_QLD'], inplace=True)\n",
    "if '_SA' not in df:\n",
    "    df['_SA'] = df['c_SA'] + df['m_SA']\n",
    "    df.drop(columns=['c_SA', 'm_SA'], inplace=True)\n",
    "if '_TAS' not in df:\n",
    "    df['_TAS'] = df['c_TAS'] + df['m_TAS']\n",
    "    df.drop(columns=['c_TAS', 'm_TAS'], inplace=True)\n",
    "if '_VIC' not in df:\n",
    "    df['_VIC'] = df['c_VIC'] + df['m_VIC']\n",
    "    df.drop(columns=['c_VIC', 'm_VIC'], inplace=True)\n",
    "if '_WA' not in df:\n",
    "    df['_WA'] = df['c_WA'] + df['m_WA']\n",
    "    df.drop(columns=['c_WA', 'm_WA'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Identify annual salary of each customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Subset Pay/Salary transactions and group by account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display counts of payment types\n",
    "total_sal = df.amt[df.t_pay == 1].groupby(df.acc).sum()\n",
    "annual_sal = total_sal * 4\n",
    "annual_sal = annual_sal.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('acc').mean()\n",
    "df['a_sal'] = annual_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature selection and data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[list(set(df.columns) - set(df[['a_sal']]))]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "y = df.a_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 15))\n",
    "sns.boxplot(data=X, orient='h', palette=\"cool_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(30,20))\n",
    "_ = sns.heatmap(df.corr(), vmin=0, vmax=1, center=0, ax=ax, annot=True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build linear regression model to predict annual salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_coeff(X, model):\n",
    "    model_coefs = pd.DataFrame({'variable': X.columns,\n",
    "                                'coef': model.coef_,\n",
    "                                'abs_coef': np.abs(model.coef_)})\n",
    "    model_coefs.sort_values('abs_coef', inplace=True, ascending=False)\n",
    "    sns.barplot(x=\"variable\", y=\"coef\", data=model_coefs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Coefficients\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "view_coeff(X, reg)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear regression tends to only work on low dimensional datasets. From the plot above it is clear that the linear model is influenced by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers to see whether it improves accuracy\n",
    "df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[list(set(df.columns) - set(df[['a_sal']]))]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "y = df.a_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Coefficients\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "view_coeff(X, reg)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Therefore, removing outliers does improve accuracy. However, that accuracy score is a very low baseline. We should try to improve our accuracy by using other models and foward feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Ridge Regression alpha optimization - L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Calculate Ridge Regression model\n",
    "\n",
    "# create a model object to hold the modelling parameters\n",
    "clf = Ridge()\n",
    "\n",
    "# keep track of the intermediate results for coefficients and errors\n",
    "coefs = []\n",
    "errors = []\n",
    "\n",
    "# create a range of alphas to calculate\n",
    "ridge_alphas = np.logspace(-6, 6, 200)\n",
    "\n",
    "# Train the model with different regularisation strengths\n",
    "for a in ridge_alphas:\n",
    "    clf.set_params(alpha = a)\n",
    "    clf.fit(X, y)\n",
    "    coefs.append(clf.coef_)\n",
    "    errors.append(mean_squared_error(clf.coef_, reg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optimal_ridge = RidgeCV(alphas=ridge_alphas, cv=10)\n",
    "optimal_ridge.fit(X, y)\n",
    "print('Alpha:', optimal_ridge.alpha_)\n",
    "print('Score:', optimal_ridge.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "view_coeff(X, optimal_ridge)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Lasso Regression alpha optimization - L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Calculate Lasso Regression model\n",
    "\n",
    "# create a model object to hold the modelling parameters\n",
    "clf = Lasso()\n",
    "# keep track of the intermediate results for coefficients and errors\n",
    "coefs = []\n",
    "errors = []\n",
    "\n",
    "# create a range of alphas to calculate\n",
    "lasso_alphas = np.logspace(-6, 6, 200)\n",
    "# Train the model with different regularisation strengths\n",
    "for a in lasso_alphas:\n",
    "    clf.set_params(alpha = a)\n",
    "    clf.fit(X, y)\n",
    "    coefs.append(clf.coef_)\n",
    "    errors.append(mean_squared_error(clf.coef_, reg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Find Optimal Lasso Using LassoCV\n",
    "optimal_lasso = LassoCV(alphas=lasso_alphas, cv=10)\n",
    "optimal_lasso.fit(X, y)\n",
    "print('Alpha:', optimal_lasso.alpha_)\n",
    "print('Score:', optimal_lasso.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Coefficient\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "view_coeff(X, optimal_lasso)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flag intermediate output\n",
    "\n",
    "show_steps = False   # for testing/debugging\n",
    "# show_steps = False  # without showing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Use Forward Feature Selection to pick a good model\n",
    "\n",
    "# start with no predictors\n",
    "included = []\n",
    "# keep track of model and parameters\n",
    "best = {'feature': '', 'r2': 0, 'a_r2': 0}\n",
    "# create a model object to hold the modelling parameters\n",
    "model = LinearRegression()\n",
    "# get the number of cases in the test data\n",
    "n = X_test.shape[0]\n",
    "\n",
    "r2_list = []\n",
    "adjusted_r2_list = []\n",
    "\n",
    "while True:\n",
    "    changed = False\n",
    "    \n",
    "    if show_steps:\n",
    "        print('') \n",
    "\n",
    "    # list the features to be evaluated\n",
    "    excluded = list(set(X.columns) - set(included))\n",
    "    \n",
    "    if show_steps:\n",
    "        print('(Step) Excluded = %s' % ', '.join(excluded))  \n",
    "\n",
    "    # for each remaining feature to be evaluated\n",
    "    for new_column in excluded:\n",
    "        \n",
    "        if show_steps:\n",
    "            print('(Step) Trying %s...' % new_column)\n",
    "            print('(Step) - Features = %s' % ', '.join(included + [new_column]))\n",
    "\n",
    "        # fit the model with the Training data\n",
    "        fit = model.fit(X_train[included + [new_column]], y_train)\n",
    "        # calculate the score (R^2 for Regression)\n",
    "        r2 = fit.score(X_train[included + [new_column]], y_train)\n",
    "        \n",
    "        # number of predictors in this model\n",
    "        k = len(included) + 1\n",
    "        # calculate the adjusted R^2\n",
    "        adjusted_r2 = 1 - ( ( (1 - r2) * (n - 1) ) / (n - k - 1) )\n",
    "        \n",
    "        if show_steps:\n",
    "            print('(Step) - Adjusted R^2: This = %.3f; Best = %.3f' % \n",
    "                  (adjusted_r2, best['a_r2']))\n",
    "\n",
    "        # if model improves\n",
    "        if adjusted_r2 > best['a_r2']:\n",
    "            # record new parameters\n",
    "            best = {'feature': new_column, 'r2': r2, 'a_r2': adjusted_r2}\n",
    "            # flag that found a better model\n",
    "            changed = True\n",
    "            if show_steps:\n",
    "                print('(Step) - New Best!   : Feature = %s; R^2 = %.3f; Adjusted R^2 = %.3f' % \n",
    "                      (best['feature'], best['r2'], best['a_r2']))\n",
    "    # END for\n",
    "    \n",
    "    r2_list.append(best['r2'])\n",
    "    adjusted_r2_list.append(best['a_r2'])\n",
    "\n",
    "    # if found a better model after testing all remaining features\n",
    "    if changed:\n",
    "        # update control details\n",
    "        included.append(best['feature'])\n",
    "        excluded = list(set(excluded) - set(best['feature']))\n",
    "        print('Added feature %-4s with R^2 = %.3f and adjusted R^2 = %.3f' % \n",
    "              (best['feature'], best['r2'], best['a_r2']))\n",
    "    else:\n",
    "        # terminate if no better model\n",
    "        print('*'*50)\n",
    "        break\n",
    "\n",
    "print('')\n",
    "print('Resulting features:')\n",
    "print(', '.join(included))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain model with only foward selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['amt', 't_phb', 'm_NT']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "y = df.a_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'splitter': ['best', 'random'],\n",
    "              'max_depth' : [1,3,5,7,9,11,12],\n",
    "              'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10],\n",
    "              'max_features':['auto','log2','sqrt',None],\n",
    "              'max_leaf_nodes':[None,10,20,30,40,50,60,70,80,90] }\n",
    "reg = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "#             'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "              'n_estimators': [100, 200, 400],\n",
    "              'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#             'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "reg = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'kernel': ['linear', 'poly', 'rbf'],\n",
    "                'degree': np.arange(0,10),\n",
    "                'gamma': [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "        }\n",
    "\n",
    "reg = GridSearchCV(SVR(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
